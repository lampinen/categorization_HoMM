---
title: "category optimzation analysis"
author: "Andrew Lampinen"
output: html_document
---

```{r}
library(tidyverse)
library(Hmisc)
library(boot)
library(lme4)
library(lmerTest)
```

# data loading

```{r}
parent_dir = "../categorization_HoMM_optimizing"
subdirs = c("results_nmappingsper_8/language_HoMM")
num_runs = 5
```

```{r}
read_config = function(config_file) { 
  config = read_delim(config_file, delim="\n") %>%
    separate(`key, value`, c("key", "value"), sep=",", extra="merge") %>%
    spread(key, value) %>%
    mutate_at(c("base_train_tasks", "base_eval_tasks", "meta_class_train_tasks", "meta_class_eval_tasks", "meta_map_train_tasks", "meta_map_eval_tasks"), function(x) {
      x = gsub("\\\"|[][]| |\'", "", x)
      return(str_split(x, ","))
    } )
}
```

```{r}
load_d = function(results_dir, result_subdirs, num_runs, file_type) {
  d = data.frame()
  for (run_i in 0:(num_runs-1)) {
    for (result_subdir in result_subdirs) {
      filename = sprintf("%s/%s/run%i_%s.csv", results_dir, result_subdir, run_i, file_type)
      print(filename)
      if (!file.exists(filename)) {
        print(paste("skipping ", filename, sep=""))
        next
      }
      if (grepl("config", file_type)) {
        this_d = read_config(filename)
      } else {
        this_d = read.csv(filename, check.names=F, header=T) 
        names(this_d) <- make.unique(names(this_d))

      }
      this_d = this_d %>%
        mutate(run = run_i,
               run_type = result_subdir)
      d = d %>%
        bind_rows(this_d)
    }
    
  }
  return(d)
}
```

```{r}
config_d = load_d(parent_dir, subdirs, num_runs, "run_config")
guess_opt_loss_d = load_d(parent_dir, subdirs, num_runs, "guess_opt_losses") %>% 
  mutate(init_type = "guess")
random_opt_loss_d = load_d(parent_dir, subdirs, num_runs, "random_init_opt_losses") %>% 
  mutate(init_type = "random_init")
arbitrary_opt_loss_d = load_d(parent_dir, subdirs, num_runs, "arbitrary_trained_opt_losses") %>% 
  mutate(init_type = "arbitrary_train_task")
centroid_opt_loss_d = load_d(parent_dir, subdirs, num_runs, "trained_centroid_opt_losses") %>% 
  mutate(init_type = "trained_centroid")

loss_d = bind_rows(guess_opt_loss_d, random_opt_loss_d, arbitrary_opt_loss_d, centroid_opt_loss_d)
guess_opt_loss_d = data.frame()
random_opt_loss_d = data.frame()
arbitrary_opt_loss_d = data.frame()
centroid_opt_loss_d = data.frame()
```

# some manipulation

```{r}
loss_d = loss_d %>%
  gather(task_and_train_or_eval, loss, -epoch, -run, -run_type, -init_type) %>%
  separate(task_and_train_or_eval, c("task", "loss_or_accuracy", "train_or_eval"), sep="[_:]") %>%
  mutate(train_or_eval = sub("\\.[0-9]+", "", train_or_eval),
         num_train_mappings_per = str_extract(run_type, "[0-9]+"))
  
```


# basic plots
```{r}
theme_set(theme_classic())
```

```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval",
                loss_or_accuracy == "loss"),
       aes(x=epoch, y=log(loss), color=init_type)) +
  geom_line(stat="summary",
            fun.y="mean") +
  facet_wrap(run_type + run ~ .)
```

```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval",
                epoch <= 500,
                run < 5, 
                num_train_mappings_per == 8,
                loss_or_accuracy == "accuracy") %>%
         mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping output", "Centroid of tasks", "Arbitrary train task", "Random vector", "Untrained model"))),
       aes(x=epoch, y=loss, color=init_type)) +
  geom_line(aes(group=interaction(run, init_type)),
            stat="summary",
            fun.y="mean",
            na.rm=T,
            alpha=0.4) +
  geom_line(stat="summary",
            fun.y="mean",
            na.rm=T,
            size = 2) +
  geom_hline(yintercept=0.5, alpha=0.5, linetype=2) +
  scale_color_manual(values=c("#e41a1c", "#ff7f00", "#984ea3", "#477ec8", "#4daf4a")) +
  annotate("text", x=400, y=0.52, alpha=0.5, label="Chance") +
  labs(x="Epoch (training task embeddings on new data)", y="Accuracy on new tasks") +
  guides(color=guide_legend(title="")) +

#ggsave("../../../psych/dissertation/5-timescales/figures/category_optimization_curves.png", width=6, height=4)
```

```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval",
                num_train_mappings_per == 8,
                loss_or_accuracy == "loss") %>%
         mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping output", "Centroid of tasks", "Arbitrary train task", "Random vector", "Untrained model"))),
       aes(x=epoch, y=loss, color=init_type)) +
  geom_line(stat="summary",
            fun.y="mean",
            na.rm=T,
            size = 2) +
  geom_line(aes(group=interaction(run, init_type)),
            stat="summary",
            fun.y="mean",
            na.rm=T,
            alpha=0.4) +
  scale_color_manual(values=c("#e41a1c", "#ff7f00", "#984ea3", "#477ec8", "#4daf4a")) +
  labs(x="Epoch (training task embeddings on new data)", y="Loss on new tasks") +
  guides(color=guide_legend(title="")) +
  scale_y_log10(breaks=c(1e-5, 1e-4, 1e-03, 1e-02, 1e-01, 1, 1e1, 1e2), labels = c("0.00001", "0.0001", 0.001, 0.01, 0.1, 1, 10, 100))

#ggsave("../../psych/dissertation/5-timescales/figures/category_optimization_curves.png", width=6, height=4)
```

```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval",
                num_train_mappings_per == 8,
                loss_or_accuracy == "accuracy") %>%
         group_by(init_type, run, task) %>%
         summarise(regret_int=2 * sum(1.-loss)) %>%  # 2 is dt
         group_by(init_type, run) %>%
         summarise(mean_lri=mean(regret_int, na.rm=T)) %>%
         ungroup() %>%
         mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping\noutput", "Centroid\nof tasks", "Arbitrary\ntrain task", "Random\nvector", "Untrained model"))),
       aes(x=init_type, y=mean_lri, color=init_type)) +
  geom_point(stat="summary",
             fun.y="mean",
             size=2) +
  geom_errorbar(stat="summary",
                fun.data="mean_cl_boot",
                width=0.5) +
  scale_color_manual(values=c("#e41a1c", "#ff7f00", "#984ea3", "#477ec8", "#4daf4a")) +
  scale_y_log10(breaks=c(1e-01, 1, 1e1, 1e2), labels = c(0.1, 1, 10, 100)) +
  labs(x="Initialization", y="Mean cumulative regret on new task") +
  guides(color=F)

#ggsave("../../../psych/dissertation/5-timescales/figures/category_optimization_cumulative_regret.png", width=6, height=4)
```

```{r}
intermediate_data = loss_d %>%
  filter(train_or_eval == "eval",
         loss_or_accuracy == "accuracy",
         num_train_mappings_per == 8,
         !is.na(loss)) %>%
  group_by(init_type, run, task) %>%
  summarise(regret_int= 2 * sum(1.- loss)) %>%  # 2 is dt
  group_by(init_type, run) %>%
  summarise(mean_lri=mean(regret_int, na.rm=T)) 

intermediate_data %>%
  group_by(init_type) %>%
  summarise(mean_lri=mean(mean_lri))
```

```{r}
set.seed(0)  # reproducibility
CI_data = intermediate_data %>%
  group_by(init_type) %>%
  do(result=boot.ci(boot(., function(x, inds) {return(mean(x[inds,]$mean_lri))}, R=5000))) %>%
  mutate(CI_low=result$percent[4], CI_high=result$percent[5])
CI_data
```

# Analyses

```{r}
lmer(
  mean_lri ~ init_type + (1 | run),
  data=intermediate_data %>%
    filter(init_type %in% c("guess", "random_init"))
     ) %>%
  summary()
```

# MM paper plot


```{r}

ggplot(loss_d %>%
         filter(train_or_eval == "eval",
                epoch <= 500,
                num_train_mappings_per == 8,
                loss_or_accuracy == "accuracy") %>%
         mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping output", "Centroid of tasks", "Arbitrary train task", "Random vector", "Untrained model"))),
       aes(x=epoch, y=loss, color=init_type)) +
  annotate("text", x=22.5, y=0.48, alpha=0.5, label="Chance") +
  annotate("text", x=23, y=1.027, alpha=0.5, label="Optimal") +
  geom_hline(yintercept=1, linetype=2, alpha=0.5) +
  geom_hline(yintercept=0.5, linetype=3, alpha=0.5) +
  geom_line(aes(group=interaction(run, init_type)),
            stat="summary",
            fun.y="mean",
            na.rm=T,
            alpha=0.2) +
  geom_line(stat="summary",
            fun.y="mean",
            na.rm=T,
            size = 2) +
  #scale_color_manual(values=c("#e41a1c", "#ff7f00", "#984ea3", "#477ec8", "#4daf4a")) +
  scale_color_manual(values=c("#762a83", "#a6dba0", "#6abe71", "#3a9e51", "#505050")) +
  scale_y_continuous(breaks=c(0.5, 0.75, 1), labels=c("50%", "75%", "100%")) +
  labs(x="Epoch (training task representations on new data)", y="Accuracy on new tasks") +
  guides(color=guide_legend(title=NULL)) +
  theme(legend.position=c(0.775, 0.25),
        legend.key.height = unit(0.66, 'lines'),
        legend.key.width = unit(1, 'lines')) 

ggsave("../../metamapping_paper/figures/category_optimization_curves.png", width=4, height=3)
```