---
title: "Polynomial optimzation analysis"
author: "Andrew Lampinen"
output: html_document
---

```{r}
library(tidyverse)
library(Hmisc)
library(boot)
```

# data loading

```{r}
parent_dir = "../categorization_HoMM_optimizing"
subdirs = c("results_nmappingsper_4/language_HoMM")
num_runs = 5
```

```{r}
read_config = function(config_file) { 
  config = read_delim(config_file, delim="\n") %>%
    separate(`key, value`, c("key", "value"), sep=",", extra="merge") %>%
    spread(key, value) %>%
    mutate_at(c("base_train_tasks", "base_eval_tasks", "meta_class_train_tasks", "meta_class_eval_tasks", "meta_map_train_tasks", "meta_map_eval_tasks"), function(x) {
      x = gsub("\\\"|[][]| |\'", "", x)
      return(str_split(x, ","))
    } )
}
```

```{r}
load_d = function(results_dir, result_subdirs, num_runs, file_type) {
  d = data.frame()
  for (run_i in 0:(num_runs-1)) {
    for (result_subdir in result_subdirs) {
      filename = sprintf("%s/%s/run%i_%s.csv", results_dir, result_subdir, run_i, file_type)
      print(filename)
      if (!file.exists(filename)) {
        print(paste("skipping ", filename, sep=""))
        next
      }
      if (grepl("config", file_type)) {
        this_d = read_config(filename)
      } else {
        this_d = read.csv(filename, check.names=F, header=T) 
        names(this_d) <- make.unique(names(this_d))

      }
      this_d = this_d %>%
        mutate(run = run_i,
               run_type = result_subdir)
      d = d %>%
        bind_rows(this_d)
    }
    
  }
  return(d)
}
```

```{r}
config_d = load_d(parent_dir, subdirs, num_runs, "run_config")
guess_opt_loss_d = load_d(parent_dir, subdirs, num_runs, "guess_opt_losses") %>% 
  mutate(init_type = "guess")
random_opt_loss_d = load_d(parent_dir, subdirs, num_runs, "random_init_opt_losses") %>% 
  mutate(init_type = "random_init")
arbitrary_opt_loss_d = load_d(parent_dir, subdirs, num_runs, "arbitrary_trained_opt_losses") %>% 
  mutate(init_type = "arbitrary_train_task")
centroid_opt_loss_d = load_d(parent_dir, subdirs, num_runs, "trained_centroid_opt_losses") %>% 
  mutate(init_type = "trained_centroid")

loss_d = bind_rows(guess_opt_loss_d, random_opt_loss_d, arbitrary_opt_loss_d, centroid_opt_loss_d)
guess_opt_loss_d = data.frame()
random_opt_loss_d = data.frame()
arbitrary_opt_loss_d = data.frame()
centroid_opt_loss_d = data.frame()
```

# some manipulation

```{r}
loss_d = loss_d %>%
  gather(task_and_train_or_eval, loss, -epoch, -run, -run_type, -init_type) %>%
  separate(task_and_train_or_eval, c("task", "loss_or_accuracy", "train_or_eval"), sep="[_:]") %>%
  mutate(train_or_eval = sub("\\.[0-9]+", "", train_or_eval))
  
```


# basic plots
```{r}
theme_set(theme_classic())
```

```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval",
                loss_or_accuracy == "loss"),
       aes(x=epoch, y=log(loss), color=init_type)) +
  geom_line(stat="summary",
            fun.y="mean") +
  facet_wrap(run_type + run ~ .)
```

```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval",
                loss_or_accuracy == "accuracy") %>%
         mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping output", "Centroid of tasks", "Arbitrary train task", "Random vector", "Untrained model"))),
       aes(x=epoch, y=loss, color=init_type)) +
  geom_line(stat="summary",
            fun.y="mean",
            na.rm=T,
            size = 2) +
  geom_line(aes(group=interaction(run, init_type)),
            stat="summary",
            fun.y="mean",
            na.rm=T,
            alpha=0.4) +
  geom_hline(yintercept=0.5, alpha=0.5, linetype=2) +
  scale_color_manual(values=c("#e41a1c", "#ff7f00", "#984ea3", "#477ec8", "#4daf4a")) +
  annotate("text", x=800, y=0.55, alpha=0.5, label="Chance") +
  labs(x="Epoch (training task embeddings on new data)", y="Accuracy on new tasks") +
  guides(color=guide_legend(title="")) 

#ggsave("../../psych/dissertation/5-timescales/figures/polynomial_optimization_curves.png", width=6, height=4)
```

```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval",
                loss_or_accuracy == "loss") %>%
         mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping output", "Centroid of tasks", "Arbitrary train task", "Random vector", "Untrained model"))),
       aes(x=epoch, y=loss, color=init_type)) +
  geom_line(stat="summary",
            fun.y="mean",
            na.rm=T,
            size = 2) +
  geom_line(aes(group=interaction(run, init_type)),
            stat="summary",
            fun.y="mean",
            na.rm=T,
            alpha=0.4) +
  scale_color_manual(values=c("#e41a1c", "#ff7f00", "#984ea3", "#477ec8", "#4daf4a")) +
  labs(x="Epoch (training task embeddings on new data)", y="Loss on new tasks") +
  guides(color=guide_legend(title="")) +
  scale_y_log10(breaks=c(1e-5, 1e-4, 1e-03, 1e-02, 1e-01, 1, 1e1, 1e2), labels = c(0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100))

#ggsave("../../psych/dissertation/5-timescales/figures/polynomial_optimization_curves.png", width=6, height=4)
```

```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval",
                loss_or_accuracy == "accuracy") %>%
         group_by(init_type, run, task) %>%
         summarise(regret_int=sum(1.-loss)) %>%
         group_by(init_type, run) %>%
         summarise(mean_lri=mean(regret_int, na.rm=T)) %>%
         ungroup() %>%
         mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping\noutput", "Centroid\nof tasks", "Arbitrary\ntrain task", "Random\nvector", "Untrained model"))),
       aes(x=init_type, y=mean_lri, fill=init_type)) +
  geom_bar(stat="summary",
           fun.y="mean") +
  geom_errorbar(stat="summary",
                fun.data="mean_cl_boot",
                width=0.5) +
  scale_fill_manual(values=c("#e41a1c", "#ff7f00", "#984ea3", "#477ec8", "#4daf4a")) +
  labs(x="Initialization", y="Mean cumulative regret on new task") +
  guides(fill=F)

#ggsave("../../psych/dissertation/5-timescales/figures/polynomial_optimization_cumulative_regret.png", width=6, height=4)
```
```{r}
intermediate_data = loss_d %>%
  filter(train_or_eval == "eval",
         loss_or_accuracy == "accuracy",
         !is.na(loss)) %>%
  group_by(init_type, run, task) %>%
  summarise(regret_int=sum(1.- loss)) %>%
  group_by(init_type, run) %>%
  summarise(mean_lri=mean(regret_int, na.rm=T)) 

intermediate_data %>%
  group_by(init_type) %>%
  summarise(mean_lri=mean(mean_lri))
```

```{r}
set.seed(0)  # reproducibility
CI_data = intermediate_data %>%
  group_by(init_type) %>%
  do(result=boot.ci(boot(., function(x, inds) {return(mean(x[inds,]$mean_lri))}, R=5000))) %>%
  mutate(CI_low=result$percent[4], CI_high=result$percent[5])
CI_data
```

## MM paper plots

TODO: update these
```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval") %>%
         mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping output", "Centroid of tasks", "Arbitrary train task", "Random vector", "Untrained model"))),
       aes(x=epoch, y=loss, color=init_type)) +
  geom_line(stat="summary",
            fun.y="mean",
            na.rm=T,
            size = 2) +
  geom_line(aes(group=interaction(run, init_type)),
            stat="summary",
            fun.y="mean",
            na.rm=T,
            alpha=0.2) +
  geom_hline(yintercept=log(chance), alpha=0.5, linetype=2) +
  scale_color_manual(values=c("#e41a1c", "#ff7f00", "#984ea3", "#477ec8", "#4daf4a")) +
  annotate("text", x=240, y=log(chance) - 1, alpha=0.5, label="Chance") +
  labs(x="Epoch (training task embeddings on new data)", y="Loss on new tasks") +
  guides(color=guide_legend(title="")) +
  scale_y_log10(breaks=c(1e-02, 1e-01, 1, 1e1, 1e2), labels = c(0.01, 0.1, 1, 10, 100)) +
  theme(legend.position=c(0.8, 0.875),
        legend.key.size = unit(0.5, 'lines'))

#ggsave("../metamapping_paper/figures/optimization_curves.png", width=4, height=3)
```

```{r}
ggplot(loss_d %>%
         filter(train_or_eval == "eval",
                !is.na(loss),
                init_type != "untrained") %>%
         group_by(init_type, run, task) %>%
         summarise(regret_int=sum(loss)) %>%
         group_by(init_type, run) %>%
         summarise(mean_lri=mean(regret_int, na.rm=T)) %>%
         ungroup() %>%
         mutate(init_type=factor(init_type, levels=c("guess", "trained_centroid", "arbitrary_train_task", "random_init", "untrained"), labels=c("Meta-mapping\noutput", "Centroid\nof tasks", "Arbitrary\ntrain task", "Random\nvector", "Untrained model"))),
       aes(x=init_type, y=mean_lri, fill=init_type)) +
  geom_bar(stat="summary",
           fun.y="mean") +
  geom_errorbar(stat="summary",
                fun.data="mean_cl_boot",
                width=0.5) +
  scale_fill_manual(values=c("#e41a1c", "#ff7f00", "#984ea3", "#477ec8", "#4daf4a")) +
  labs(x="Task representation initialization", y="Mean cumulative loss on new task") +
  guides(fill=F)

```
